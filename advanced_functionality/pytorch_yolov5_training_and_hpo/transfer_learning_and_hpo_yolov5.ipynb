{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6b4de3",
   "metadata": {},
   "source": [
    "# Transfer Learning and Hyperparameter Optimization for YOLOv5 using Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c1444",
   "metadata": {},
   "source": [
    "YOLO (You Only Look Once) is one of a family of models that is used for object detection. Object detection is a form of ML solution in computer vision (CV) where a neural network predicts the presence of certain class of objects within an image and points out their location in the picture through the use of bounding boxes.\n",
    "\n",
    "There are broadly two approaches that are commonly adopted for this task, a two step approach and a single shot approach, [YOLO](https://arxiv.org/abs/1506.02640) comes under the family of appraoches that leverages the single shot approach. The other is the [Single Shot Multibox Detector](https://arxiv.org/pdf/1512.02325.pdf) (SSD), [available built-in with Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14388f63",
   "metadata": {},
   "source": [
    "This notebook will take you through how to use Amazon SageMaker for transfer learning a YOLOv5 model on custom data. You will be using AWS [Deep Learning Containers (DLC)](https://github.com/aws/deep-learning-containers/blob/master/available_images.md) to customise the training container so you can use transfer learning to train a YOLOv5 model to detect car licence plates (custom data). If you are using a Amazon SageMaker Notebook instance, you should use the **Python 3** kernel, if you are in a SageMaker Studio environment use the **Python 3 Data Science** kernel.\n",
    "\n",
    "You will also use Amazon SageMaker's Automated Model Tuning feature to perform hyperparameter optimisation to arrive at the best possible model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ff4c7",
   "metadata": {},
   "source": [
    "You will be using this [car licence plate detection dataset from Kaggle](https://www.kaggle.com/datasets/andrewmvd/car-plate-detection). This would be applicable to a fairly common use case like keeping track of cars in a parking lot, or tracking cars as theey pass through toll booths. The YOLOv5 model fits the use case just right.\n",
    "\n",
    "Broadly, you will be covering the following topics in this notebook:\n",
    "\n",
    "1. [Pre-requisites if you are in a SageMaker Studio environment](#Pre-requisites-if-you-are-in-a-SageMaker-Studio-environment)\n",
    "\n",
    "2. [Data Preparation](#Data-Preparation)\n",
    "    \n",
    "    a. [Converting from VOC Pascal to .txt](#Converting-from-VOC-Pascal-to-.txt)\n",
    "    \n",
    "    b. [Converting from Amazon SageMaker Ground Truth bounding box to .txt [OPTIONAL]](#Converting-from-Amazon-SageMaker-Ground-Truth-bounding-box-to-.txt-[OPTIONAL])\n",
    "    \n",
    "    c. [Training and validation sets](#Training-and-validation-sets)\n",
    "    \n",
    "    \n",
    "3. [Set up the PyTorch environment for training](#Set-up-the-PyTorch-environment-for-training)\n",
    "\n",
    "    a. [Customising a DLC for training](#Customising-a-DLC-for-training)\n",
    "    \n",
    "    b. [Get and then upload weights to S3 location](#Get-and-then-upload-weights-to-S3-location)\n",
    "    \n",
    "\n",
    "4. [Training](#Training)\n",
    "\n",
    "\n",
    "5. [Hyperparameter Optimization with Amazon SageMaker Automated Model Tuning](#Hyperparameter-Optimization-with-Amazon-SageMaker-Automated-Model-Tuning)\n",
    "\n",
    "\n",
    "6. [Conclusion : Deploying YOLOv5 on Amazon SageMaker](#Conclusion-:-Deploying-YOLOv5-on-Amazon-SageMaker)\n",
    "\n",
    "\n",
    "\n",
    "To begin with, you will start by downloading the dataset by clicking the **Download button** on [this page](https://www.kaggle.com/datasets/andrewmvd/car-plate-detection), look for it on the top right, alternatively you could also use [the Kaggle API](https://github.com/Kaggle/kaggle-api). Upload the downloaded file ```archive.zip``` to your working directory on this SageMaker notebook instance. Once you have it on the notebook instance follow the instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b615936",
   "metadata": {},
   "source": [
    "## Pre-requisites if you are in a SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49d7bc",
   "metadata": {},
   "source": [
    "**Check your SageMaker execution role** if not already, ensure that you have the following trust policy with AWS Codebuild in place:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": [\n",
    "          \"codebuild.amazonaws.com\"\n",
    "        ]\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "You can use this guidance on [editing trust relationship for an existing role](https://docs.aws.amazon.com/directoryservice/latest/admin-guide/edit_trust.html) to edit the trust policy for your SageMaker execution role.\n",
    "\n",
    "Add this inline policy, you can use this guidance from the AWS IAM documentation to [add a permission policy inline](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html#add-policies-console) to your SageMaker execution role.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"codebuild:DeleteProject\",\n",
    "                \"codebuild:CreateProject\",\n",
    "                \"codebuild:BatchGetBuilds\",\n",
    "                \"codebuild:StartBuild\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:codebuild:*:*:project/sagemaker-studio*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"logs:CreateLogStream\",\n",
    "            \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/codebuild/sagemaker-studio*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:GetLogEvents\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/codebuild/sagemaker-studio*:log-stream:*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"logs:CreateLogGroup\",\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"ecr:CreateRepository\",\n",
    "                \"ecr:BatchGetImage\",\n",
    "                \"ecr:CompleteLayerUpload\",\n",
    "                \"ecr:DescribeImages\",\n",
    "                \"ecr:DescribeRepositories\",\n",
    "                \"ecr:UploadLayerPart\",\n",
    "                \"ecr:ListImages\",\n",
    "                \"ecr:InitiateLayerUpload\",\n",
    "                \"ecr:BatchCheckLayerAvailability\",\n",
    "                \"ecr:PutImage\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:ecr:*:*:repository/sagemaker-studio*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"ecr:GetAuthorizationToken\",\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "              \"s3:GetObject\",\n",
    "              \"s3:DeleteObject\",\n",
    "              \"s3:PutObject\"\n",
    "              ],\n",
    "            \"Resource\": \"arn:aws:s3:::sagemaker-*/*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:CreateBucket\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::sagemaker*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:GetRole\",\n",
    "                \"iam:ListRoles\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"arn:aws:iam::*:role/*\",\n",
    "            \"Condition\": {\n",
    "                \"StringLikeIfExists\": {\n",
    "                    \"iam:PassedToService\": \"codebuild.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "You can now proceed with executing the rest of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddabd7ca",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/Sample-pytorch-YOLOv5-LP-Detection\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"Bucket Name: {} and the role is {}\".format(bucket, role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f18bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import xml.etree.ElementTree as ET\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sagemaker import image_uris\n",
    "from zipfile import ZipFile\n",
    "from botocore.exceptions import ClientError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "SAGEMAKERWDIR = os.getcwd() + \"/\"\n",
    "DATASETLOCALBASE = SAGEMAKERWDIR + \"data\"\n",
    "ANNOTATIONSPATH = DATASETLOCALBASE + \"/annotations\"\n",
    "YOLO5ANNOTATIONS = ANNOTATIONSPATH + \"/YOLOv5\"\n",
    "IMAGESPATH = DATASETLOCALBASE + \"/images\"\n",
    "\n",
    "\n",
    "def unzipdataset(wrkdir, archname):\n",
    "    with ZipFile(archname, \"r\") as arch:\n",
    "        if not os.path.exists(DATASETLOCALBASE):\n",
    "            os.makedirs(DATASETLOCALBASE)\n",
    "        arch.extractall(path=DATASETLOCALBASE)\n",
    "        print(\"Done extracting zip archive!\")\n",
    "\n",
    "\n",
    "def readannotfile(givenfile, silent=False):\n",
    "    givenfile = ANNOTATIONSPATH + \"/\" + givenfile\n",
    "    if silent == False:\n",
    "        try:\n",
    "            with open(givenfile, \"r\") as fle:\n",
    "                for line in fle:\n",
    "                    print(line)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            return False\n",
    "    return os.path.exists(\n",
    "        IMAGESPATH + \"/\" + ((os.path.basename(givenfile)).split(\".\")[-2]) + \".png\"\n",
    "    )\n",
    "\n",
    "\n",
    "def parseVOCPascalFile(xmlfilename):\n",
    "    if readannotfile(ANNOTATIONSPATH + \"/\" + xmlfilename, silent=True):\n",
    "        tree = ET.parse(ANNOTATIONSPATH + \"/\" + xmlfilename)\n",
    "        root = tree.getroot()\n",
    "        completeflname, clas, xcenter, ycenter, width, height = \"\", 0, 0.0, 0.0, 0.0, 0.0\n",
    "        ## We are only interested in filename, object class, xmin, ymin, xmax, ymax, width, height\n",
    "        for filename in root.findall(\"filename\"):\n",
    "            fname = filename.text\n",
    "            flname = ((fname.split(\".\"))[-2]) + \".txt\"\n",
    "            if not os.path.exists(YOLO5ANNOTATIONS):\n",
    "                os.makedirs(YOLO5ANNOTATIONS)\n",
    "            completeflname = YOLO5ANNOTATIONS + \"/\" + flname\n",
    "        for obj in root.findall(\"object\"):\n",
    "            cname = obj.find(\"name\").text\n",
    "            if cname == \"licence\":\n",
    "                clas = 0\n",
    "            xmin = float(obj.find(\"bndbox\").find(\"xmin\").text)\n",
    "            ymin = float(obj.find(\"bndbox\").find(\"ymin\").text)\n",
    "            xmax = float(obj.find(\"bndbox\").find(\"xmax\").text)\n",
    "            ymax = float(obj.find(\"bndbox\").find(\"ymax\").text)\n",
    "        for size in root.findall(\"size\"):\n",
    "            input_width = float(size.find(\"width\").text)\n",
    "            input_height = float(size.find(\"height\").text)\n",
    "        xcenter = (xmin + (xmax - xmin) / 2.0) / input_width\n",
    "        ycenter = (ymin + (ymax - ymin) / 2.0) / input_height\n",
    "        width = (xmax - xmin) / input_width\n",
    "        height = (ymax - ymin) / input_height\n",
    "\n",
    "        ## writing to the new annotation file\n",
    "        f = open(YOLO5ANNOTATIONS + \"/\" + flname, \"w\")\n",
    "        print(\"Writing to {}\".format(completeflname))\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = f\n",
    "        print(\"{} {} {} {} {}\".format(clas, xcenter, ycenter, width, height))\n",
    "        sys.stdout = original_stdout\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def plot_bounding_box(imagefile, annotationfile):\n",
    "    assert os.path.exists(imagefile)\n",
    "    image = Image.open(imagefile)\n",
    "\n",
    "    assert os.path.exists(imagefile)\n",
    "    f = open(annotationfile, \"r\")\n",
    "    bx = \"\"\n",
    "    for line in f:\n",
    "        bx = line  # We want to teest a single box in any image\n",
    "    f.close()\n",
    "\n",
    "    annotation_list = bx.strip().split(\" \")\n",
    "\n",
    "    annotations = np.array(annotation_list, dtype=np.float64)\n",
    "\n",
    "    w, h = image.size\n",
    "\n",
    "    plotted_image = ImageDraw.Draw(image)\n",
    "\n",
    "    chged_annots = np.array(annotations)\n",
    "    chged_annots[1] = annotations[1] * w\n",
    "    chged_annots[2] = annotations[2] * h\n",
    "    chged_annots[3] = annotations[3] * w\n",
    "    chged_annots[4] = annotations[4] * h\n",
    "    chged_annots[1] = chged_annots[1] - chged_annots[3] / 2  # xmin\n",
    "    chged_annots[2] = chged_annots[2] - chged_annots[4] / 2  # ymin\n",
    "    chged_annots[3] = chged_annots[1] + chged_annots[3]  # xmax\n",
    "    chged_annots[4] = chged_annots[2] + chged_annots[4]  # ymax\n",
    "\n",
    "    xmin, ymin, xmax, ymax = chged_annots[1], chged_annots[2], chged_annots[3], chged_annots[4]\n",
    "\n",
    "    plotted_image.rectangle(((xmin, ymin), (xmax, ymax)), outline=\"red\", width=2)\n",
    "\n",
    "    plt.imshow(np.array(image))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9df945",
   "metadata": {},
   "outputs": [],
   "source": [
    "unzipdataset(SAGEMAKERWDIR, \"archive.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d416b27",
   "metadata": {},
   "source": [
    "Let's read one of the annotations files, these files are in the PASCAL VOC format. Lets see what we need to do to process them. We have change it to the .txt format that we will use for the YOLOv5 model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is an example of a file that we have read\n",
    "readannotfile(\"Cars234.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ab449",
   "metadata": {},
   "source": [
    "Also, let us make sure that our data is good, one of the things we do is make sure that we have an xml file for every png image file we have. You should not get any output for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58062c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fyle in os.listdir(ANNOTATIONSPATH):\n",
    "    valid = readannotfile(fyle, silent=True)\n",
    "    if not valid:\n",
    "        print(\"Filename: {}, invalid data: {}\".format(fyle, str(valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a0c30",
   "metadata": {},
   "source": [
    "### Converting from VOC Pascal to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for annots in os.listdir(ANNOTATIONSPATH):\n",
    "    parseVOCPascalFile(annots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d944cf",
   "metadata": {},
   "source": [
    "Let us see if we have got the converted annotations right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4273ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A test\n",
    "plot_bounding_box(IMAGESPATH + \"/Cars250.png\", YOLO5ANNOTATIONS + \"/Cars250.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2745d06",
   "metadata": {},
   "source": [
    "One of the things we noticed about our dataset, is that this dataset has images of variable sizes, good for us YOLO accomodates this, and resizes the images, and defaults to 640. In fact, when we train on our custom dataset, the image size will default to 640."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c80f4",
   "metadata": {},
   "source": [
    "### Converting from Amazon SageMaker Ground Truth bounding box to .txt  [OPTIONAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35e4cf",
   "metadata": {},
   "source": [
    "You can use [Amazon SageMaker Ground Truth](https://aws.amazon.com/sagemaker/data-labeling/) to build your data labeling workflows, it is a data labeling service that enables you to use [Amazon Mechanical Turk](https://www.mturk.com/), third party vendors, or your own private workforce for your data labeling tasks. SageMaker Ground Truth can be used to label images, text, videos and video frames, as well as 3D point clouds. It can also generate labeled synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5507e29",
   "metadata": {},
   "source": [
    "If you are using one of SageMaker Ground Truth, you can create a [bounding box labeling job](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-bounding-box.html), this notebook will demonstrate how you can transform the output from such a job to input suitable for your YOLOv5 training job. Please refer to the [SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-bounding-box.html) for guidance on how you can create a bounding box labeling job. \n",
    "\n",
    "Below is a sample output line from such a labeling job, formatted for easy reading. This would be one line in the ```output.manifest``` file at the end of the labeling job as the output manifest is generated in the [JSONlines format](https://jsonlines.org/). Along with this notebook you have been provided a sample output manifest file named ```sample_output.manifest```. This sample Ground Truth labeling job output file will be used to demonstrate how you can transform annotations to a format appropriate for YOLOv5 training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d5fe30",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "    \"source-ref\": \"s3://rns-groundtruth-bucket/ground-truth-od-full-demo/images/000062a39995e348.jpg\",\n",
    "    \"category\": {\n",
    "        \"image_size\": [\n",
    "            {\n",
    "                \"width\": 680,\n",
    "                \"height\": 1024,\n",
    "                \"depth\": 3\n",
    "            }\n",
    "        ],\n",
    "        \"annotations\": [\n",
    "            {\n",
    "                \"class_id\": 0,\n",
    "                \"top\": 164.60000000000002,\n",
    "                \"left\": 138.8,\n",
    "                \"height\": 859,\n",
    "                \"width\": 443.8\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"category-metadata\": {\n",
    "        \"objects\": [\n",
    "            {\n",
    "                \"confidence\": 0.93\n",
    "            }\n",
    "        ],\n",
    "        \"class-map\": {\n",
    "            \"0\": \"Bird\"\n",
    "        },\n",
    "        \"type\": \"groundtruth/object-detection\",\n",
    "        \"human-annotated\": \"yes\",\n",
    "        \"creation-date\": \"2022-09-12T09:51:00.148118\",\n",
    "        \"job-name\": \"labeling-job/ground-truth-od-demo-1662974840\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa85a75",
   "metadata": {},
   "source": [
    "The above output demonstrates a single class bounding box labeling job which is one json line in the output manifest. The above output was extracted from the output manifest generated after running [this notebook](https://github.com/aws/amazon-sagemaker-examples/blob/6ac5bb28dcbe29e16d3cb8fe7169cabe1c6f34eb/ground_truth_labeling_jobs/ground_truth_object_detection_tutorial/object_detection_tutorial.ipynb) using a SageMaker Notebook instance. Below is function that transforms an output.manifest file to .TXT files that the YOLOv5 training job expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36beb751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertGT2TXT(manifestfile, topannotationloc):\n",
    "    \"\"\"\n",
    "    Converts an output manifest file\n",
    "    from its existing JSONlines format\n",
    "    to the TXT format for YOLOv5\n",
    "    training.\n",
    "    manifestfile: The name of the output manifest file from the Ground Truth labeling job.\n",
    "    topannotationloc: The parent directory below which the TXT annotations files are placed.\n",
    "    \"\"\"\n",
    "    annotobj = []\n",
    "    with open(manifestfile) as mf:\n",
    "        for line in mf:\n",
    "            annotobj.append(json.loads(line))\n",
    "    for ob in annotobj:\n",
    "        ## Get the TXT filename\n",
    "        txtflname = (ob[\"source-ref\"].split(\"/\")[-1]).split(\".\")[-2] + \".txt\"\n",
    "        print(\"Writing to {}\".format(topannotationloc + \"/\" + txtflname))\n",
    "        ## Writing to the new annotation file\n",
    "        annotf = open(topannotationloc + \"/\" + txtflname, \"w\")\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = annotf\n",
    "        input_width = ob[\"category\"][\"image_size\"][0][\"width\"]\n",
    "        input_height = ob[\"category\"][\"image_size\"][0][\"height\"]\n",
    "        for annots in ob[\"category\"][\"annotations\"]:\n",
    "            clas = annots[\"class_id\"]\n",
    "            xcenter = (annots[\"left\"] + annots[\"width\"] / 2.0) / input_width\n",
    "            ycenter = (annots[\"top\"] + annots[\"height\"] / 2.0) / input_height\n",
    "            width = annots[\"width\"] / input_width\n",
    "            height = annots[\"height\"] / input_height\n",
    "            print(\"{} {} {} {} {}\".format(clas, xcenter, ycenter, width, height))\n",
    "        sys.stdout = original_stdout\n",
    "        annotf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating TXT files, assuming a directory does not exist\n",
    "!mkdir gt2txt\n",
    "convertGT2TXT(\"./sample_output.manifest\", \"gt2txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25c8d4",
   "metadata": {},
   "source": [
    "Eventually, you will need to build the same file structure with these TXT files and image files as is demonstrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9961deb",
   "metadata": {},
   "source": [
    "### Training and validation sets\n",
    "\n",
    "Here we create the training and validation sets, with 20% set aside for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc3131",
   "metadata": {},
   "outputs": [],
   "source": [
    "fyls = os.listdir(IMAGESPATH)\n",
    "random.shuffle(fyls)\n",
    "\n",
    "annots = [fyl.split(\".\")[-2] + \".txt\" for fyl in fyls]\n",
    "X_train, X_val, y_train, y_val = train_test_split(fyls, annots, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"A set of 5 training image files : \")\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a508a625",
   "metadata": {},
   "source": [
    "Right now, the data is organized like so:\n",
    "\n",
    "**Images**: ```IMAGESPATH/*.png```\n",
    "\n",
    "**Annotations**: ```YOLO5ANNOTATIONS/*.txt```\n",
    "\n",
    "We need to separate these into training and validation datasets. This is what they should look like on S3, and eventually, on the training container.\n",
    "\n",
    "**Training**: ```<base path>/train/[images|labels]```\n",
    "\n",
    "**Validation**: ```<base path>/val/[images|labels]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c68110",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3TRAINIMAGESLOC = prefix + \"/data/train/\"\n",
    "S3TRAINANNOTSLOC = prefix + \"/data/train/\"\n",
    "S3VALIMAGESLOC = prefix + \"/data/val/\"\n",
    "S3VALANNOTSLOC = prefix + \"/data/val/\"\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "for i in X_train:\n",
    "    try:\n",
    "        response = s3_client.upload_file(IMAGESPATH + \"/\" + i, bucket, S3TRAINIMAGESLOC + i)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "\n",
    "for i in X_val:\n",
    "    try:\n",
    "        response = s3_client.upload_file(IMAGESPATH + \"/\" + i, bucket, S3VALIMAGESLOC + i)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "\n",
    "for i in y_train:\n",
    "    try:\n",
    "        response = s3_client.upload_file(YOLO5ANNOTATIONS + \"/\" + i, bucket, S3TRAINANNOTSLOC + i)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "\n",
    "for i in y_val:\n",
    "    try:\n",
    "        response = s3_client.upload_file(YOLO5ANNOTATIONS + \"/\" + i, bucket, S3VALANNOTSLOC + i)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "\n",
    "TRAIN_CHANNEL = \"s3://\" + bucket + \"/\" + prefix + \"/data/train/\"\n",
    "VAL_CHANNEL = \"s3://\" + bucket + \"/\" + prefix + \"/data/val/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb83b40",
   "metadata": {},
   "source": [
    "## Set up the PyTorch environment for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb7644",
   "metadata": {},
   "source": [
    "#### Customising a DLC for training\n",
    "\n",
    "You will be creating your own container, you will be using one of the Deep Learning Containers as a base image. Look for something suitable that you can use. You can alternatively use a GPU image, you can specify the kind of image you want to use. Think of your use case when deciding on the kind of DLC image you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33352b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=\"us-east-1\",\n",
    "    version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    image_scope=\"training\",\n",
    "    instance_type=\"ml.c5.4xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize docker/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f9d98",
   "metadata": {},
   "source": [
    "#### If you are in a studio notebook environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a060780",
   "metadata": {},
   "source": [
    "**Do Not** run the next three cells below. Follow the instructions on [building and pushing your docker container in the studio notebook environment](#Building-your-docker-image-and-pushing-it-to-Amazon-EC2-Container-Registry-(ECR)-in-a-SageMaker-Studio-notebook-environment) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize docker/build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe015dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x docker/build_and_push.sh && docker/build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430bc262",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "\n",
    "algorithm_name = \"pytorch-training-container-extension-yolov5-cpu\"\n",
    "\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, algorithm_name)\n",
    "\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fc9837",
   "metadata": {},
   "source": [
    "#### Building your docker image and pushing it to Amazon EC2 Container Registry (ECR) in a SageMaker Studio notebook environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871fe4c",
   "metadata": {},
   "source": [
    "1. If you are experimenting, give your SageMaker execution role all the required permissions. You can find detailed information about the permissions required in this AWS blogpost about [building container images from your studio notebooks](https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/).\n",
    "\n",
    "2. Next, install the ```sagemaker-studio-image-build``` package using pip.\n",
    "\n",
    "3. Finally, build and register the container image using the following command:\n",
    "\n",
    "   ```sm-docker build . --file /path/to/Dockerfile```\n",
    "   \n",
    "**Run the next three cells for the SageMaker Studio environment only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31290d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69599b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sm-docker build . --file docker/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c44716",
   "metadata": {},
   "source": [
    "If you are in a studio environment use the **Image URI** from the execution of the previous cell. Use this value to populate the image_uri variable in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected format of the image URI : <<AWS_ACCOUNT_ID>>.dkr.ecr.<<REGION>>.amazonaws.com/<<ECR_REPO_NAME>>:<<SAGEMAKER_STUDIO_USER>>\n",
    "image_uri = <<Image URI value copy/pasted from the execution of the previous cell>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233713e5",
   "metadata": {},
   "source": [
    "__At this point__ in the notebook, irrespective of the environment you are running in i.e. Studio or Notebook instance, your ```image_uri``` variable should be populated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1796863e",
   "metadata": {},
   "source": [
    "Since you are tuning the model for a custom dataset, you will require weights for initialization. Pretained weights (checkpoints) can be found on the [YOLOv5 release page](https://github.com/ultralytics/yolov5/releases). Store the downloaded weights at an S3 location. For this notebook you will use the yolov5s.pt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be980163",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTSLOC = prefix + \"/data/weights/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e57d4",
   "metadata": {},
   "source": [
    "#### Get and then upload weights to S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aee1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a3e8e",
   "metadata": {},
   "source": [
    "##### Upload weights to S3 location for use during training later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97cd5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "try:\n",
    "    response = s3_client.upload_file(\"yolov5s.pt\", bucket, WEIGHTSLOC + \"yolov5s.pt\")\n",
    "except ClientError as e:\n",
    "    logging.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9020e6bb",
   "metadata": {},
   "source": [
    "#### Almost ready to initiate training\n",
    "\n",
    "A little primer about how to leverage hyperparameters to run training and automated model tuning for YOLOv5 on Amazon SageMaker.\n",
    "\n",
    "```freeze``` is used to freeze the weights of the backbone layers. The number of backbone layers will change based on the model you choose, in this case, we are freezing 10 layers, these layers serve as feature extractors. The head layers, which are **not** frozen compute the output predictions. For a guide to how you can decide on how many layers to freeze for transfer learning you may want to go through this thorough guide on [Transfer Learning with Frozen Layers](https://github.com/ultralytics/yolov5/issues/1314)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb57d0f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfde1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# JSON encode hyperparameters.\n",
    "def json_encode_hyperparameters(hyperparameters):\n",
    "    return {str(k): json.dumps(v) for (k, v) in hyperparameters.items()}\n",
    "\n",
    "\n",
    "# The values for hyperparameters are just examples and you are advised to change them to what suits your use case.\n",
    "hyperparameters = json_encode_hyperparameters(\n",
    "    {\n",
    "        \"epochs\": 5,\n",
    "        \"batchsize\": 8,\n",
    "        \"freeze\": 10,\n",
    "        \"patience\": 10,\n",
    "        \"lr0\": 0.01,\n",
    "        \"lrf\": 0.01,\n",
    "        \"weights\": \"s3://\" + bucket + \"/\" + prefix + \"/data/weights/\" + \"yolov5s.pt\",\n",
    "    }\n",
    ")\n",
    "\n",
    "training_uuid = uuid.uuid1()\n",
    "training_job_name = \"yolov5-project-\" + str(training_uuid)\n",
    "\n",
    "print(\"Starting training job : {}\".format(training_job_name))\n",
    "\n",
    "pt_estimator = PyTorch(\n",
    "    entry_point=\"yolov5/training-wrapper.py\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.c5.4xlarge\",\n",
    "    volume_size=100,\n",
    "    instance_count=1,\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    image_uri=image_uri,\n",
    "    debugger_hook_config=False,\n",
    "    output_path=\"s3://\" + bucket + \"/\" + prefix + \"/output\",\n",
    ")\n",
    "\n",
    "pt_estimator.fit(\n",
    "    {\n",
    "        \"train\": \"s3://\" + bucket + \"/\" + prefix + \"/data/train\",\n",
    "        \"val\": \"s3://\" + bucket + \"/\" + prefix + \"/data/val\",\n",
    "    },\n",
    "    job_name=training_job_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc876cc1",
   "metadata": {},
   "source": [
    "The following hyperparameters can be used during training :\n",
    "\n",
    "**lr0**: Initial learning rate\n",
    "\n",
    "**lrf**: Final OneCycleLR learning rate\n",
    "\n",
    "**momentum**: SGD momentum/Adam beta1\n",
    "\n",
    "**weight_decay**: Optimizer weight decay\n",
    "\n",
    "**warmup_epochs**: Warmup epochs \n",
    "\n",
    "**warmup_momentum**: Warmup initial momentum\n",
    "\n",
    "**warmup_bias_lr**: Warmup initial bias lr\n",
    "\n",
    "**box**: Box loss gain\n",
    "\n",
    "**cls**: cls loss gain\n",
    "\n",
    "**cls_pw**: cls BCELoss positive_weight\n",
    "\n",
    "**obj**: obj loss gain \n",
    "\n",
    "**obj_pw**: obj BCELoss positive_weight\n",
    "\n",
    "**iou_t**: IoU training threshold\n",
    "\n",
    "**anchor_t**: anchor-multiple threshold\n",
    "\n",
    "**anchors**: anchors per output grid \n",
    "\n",
    "**fl_gamma**: focal loss gamma \n",
    "\n",
    "**hsv_h**: image HSV-Hue augmentation \n",
    "\n",
    "**hsv_s**: image HSV-Saturation augmentation\n",
    "\n",
    "**hsv_v**: image HSV-Value augmentation \n",
    "\n",
    "**degrees**: image rotation \n",
    "\n",
    "**translate**: image translation \n",
    "\n",
    "**scale**: image scale \n",
    "\n",
    "**shear**: image shear \n",
    "\n",
    "**perspective**: image perspective\n",
    "\n",
    "**flipud**: image flip up-down \n",
    "\n",
    "**fliplr**: image flip left-right \n",
    "\n",
    "**mosaic**: image mosaic \n",
    "\n",
    "**mixup**: image mixup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170711d3",
   "metadata": {},
   "source": [
    "**How did the training job go?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8df3c2",
   "metadata": {},
   "source": [
    "You can get to the results.png in the model.tar.gz file that you can find at the S3 location you specified at the output path when you created the estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ff64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    bucket, prefix + \"/output/\" + str(training_job_name) + \"/output/model.tar.gz\", \"model.tar.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d93abb",
   "metadata": {},
   "source": [
    "#### Contents of the ```model.tar.gz```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4598413",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d285a4",
   "metadata": {},
   "source": [
    "We can go through the metrics and losses saved to the ```results.png```. Other relevant information can also be examined like the Precision-Recall curve, weights, F1 curve and more. Below, we show the results.png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 12), dpi=120)\n",
    "image = plt.imread(\"./exp/results.png\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceea84a",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Amazon SageMaker Automated Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e1f368",
   "metadata": {},
   "source": [
    "In this case, we will try to look at only a few metrics as a way for SageMaker to find the best model for the job, we can consider as many as 20 metrics! For guidance on how to set up the objective metrics, please refer to this documentation on [Defining Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html) for Automated Model Tuning (AMT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842fbe8",
   "metadata": {},
   "source": [
    "The YOLOv5 uses hyperparameter evolution to optimize hyperperameters. A detailed guide is provided on how this can be [achieved here](https://docs.ultralytics.com/tutorials/hyperparameter-evolution/). We will address HPO using Amazon SageMaker. The hyperparameters used by YOLOv5 are sourced from ```hyp.scratch.yaml``` file. These are as follows, below is a list of hyperparameters that can be tuned using evolution during training, along with the names and default values:\n",
    "\n",
    "```\n",
    "lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "lrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "momentum: 0.937  # SGD momentum/Adam beta1\n",
    "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
    "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
    "warmup_momentum: 0.8  # warmup initial momentum\n",
    "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
    "box: 0.05  # box loss gain\n",
    "cls: 0.5  # cls loss gain\n",
    "cls_pw: 1.0  # cls BCELoss positive_weight\n",
    "obj: 1.0  # obj loss gain (scale with pixels)\n",
    "obj_pw: 1.0  # obj BCELoss positive_weight\n",
    "iou_t: 0.20  # IoU training threshold\n",
    "anchor_t: 4.0  # anchor-multiple threshold\n",
    "anchors: 0  # anchors per output grid (0 to ignore)\n",
    "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
    "hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n",
    "hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n",
    "degrees: 0.0  # image rotation (+/- deg)\n",
    "translate: 0.1  # image translation (+/- fraction)\n",
    "scale: 0.5  # image scale (+/- gain)\n",
    "shear: 0.0  # image shear (+/- deg)\n",
    "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
    "flipud: 0.0  # image flip up-down (probability)\n",
    "fliplr: 0.5  # image flip left-right (probability)\n",
    "mosaic: 1.0  # image mosaic (probability)\n",
    "mixup: 0.0  # image mixup (probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb67f7",
   "metadata": {},
   "source": [
    "Fitness is the criterion used in YOLOv5 to arrive at the best model, and the value that is maximized. The challenge with using evolution is that it requires at least 300 generations (recommended), this can get pretty time consuming and expensive as it will need 100s or 1000s of GPU hours. Nevertheless, if you do want to use evolution, all you have to do is add --evolve when ```train.py``` is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931bea0b",
   "metadata": {},
   "source": [
    "These are the parameters you can tune:\n",
    "\n",
    "```\n",
    "lr0: continuous\n",
    "lrf: continuous\n",
    "momentum: continuous\n",
    "weight_decay: continuous\n",
    "warmup_epochs: integer\n",
    "warmup_momentum: continuous\n",
    "warmup_bias_lr: continuous\n",
    "box: continuous\n",
    "cls: continuous\n",
    "cls_pw: integer\n",
    "obj: integer\n",
    "obj_pw: integer\n",
    "iou_t: continuous\n",
    "anchor_t: integer\n",
    "anchors: integer\n",
    "fl_gamma: continuous\n",
    "hsv_h: continuous\n",
    "hsv_s: continuous\n",
    "hsv_v: continuous\n",
    "degrees: integer\n",
    "translate: continuous\n",
    "scale: continuous\n",
    "shear: integer\n",
    "perspective: continuous\n",
    "flipud: continuous\n",
    "fliplr: continuous\n",
    "mosaic: continuous\n",
    "mixup: continuous\n",
    "\n",
    "```\n",
    "\n",
    "Identify the metric you will use to evaluate. In this case you will use the objective metric of mAP@.5. You will be maximising this metric. This is a very common metric used in object detection. You can learn more about mAP and other advanced metrics in this Stanford CS230 page on [Advanced Evaluation Metrics](https://cs230.stanford.edu/section/8/). The code below demonstrates how you can perform AMT in SageMaker for tuning the YOLOv5 model for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameters = json_encode_hyperparameters(\n",
    "    {\n",
    "        \"freeze\": 10,\n",
    "        \"patience\": 10,\n",
    "        \"lr0\": 0.01,\n",
    "        \"lrf\": 0.01,\n",
    "        \"weights\": \"s3://\" + bucket + \"/\" + prefix + \"/data/weights/\" + \"yolov5s.pt\",\n",
    "    }\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"epochs\": IntegerParameter(10, 20),\n",
    "    \"batchsize\": IntegerParameter(8, 64),\n",
    "    \"iou_t\": ContinuousParameter(0.15, 0.25),\n",
    "}\n",
    "\n",
    "objective_metric_name = \"mAP.5\"\n",
    "objective_type = \"Maximize\"\n",
    "metric_definitions = [{\"Name\": \"mAP.5\", \"Regex\": \"(0\\.[0-9]{1,6}).{7,12}$\"}]\n",
    "\n",
    "tuning_uuid = uuid.uuid1()\n",
    "tuning_job_name = \"yolov5-project-hpo-\" + str(training_uuid)\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    pt_estimator,\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    objective_type=objective_type,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_jobs=3,  # Change this to a suitable number that makes sense in your case\n",
    "    max_parallel_jobs=1,  # Change this to a suitable number that makes sense in your case\n",
    "    base_tuning_job_name=tuning_job_name,\n",
    ")\n",
    "\n",
    "tuner.fit(\n",
    "    {\n",
    "        \"train\": \"s3://\" + bucket + \"/\" + prefix + \"/data/train\",\n",
    "        \"val\": \"s3://\" + bucket + \"/\" + prefix + \"/data/val\",\n",
    "    },\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea154eb",
   "metadata": {},
   "source": [
    "Now that the tuning job is complete, find the best model from the training job that gave us the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe527e9",
   "metadata": {},
   "source": [
    "Download the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    bucket,\n",
    "    prefix + \"/output/\" + tuner.best_training_job() + \"/output/model.tar.gz\",\n",
    "    \"hpo.model.tar.gz\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1868c",
   "metadata": {},
   "source": [
    "## Conclusion : Deploying YOLOv5 on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d1f685",
   "metadata": {},
   "source": [
    "Whether you simply train or perform automated model tuning (AMT), you will need create a ```model.tar.gz``` file that has to follow a specific structure, in this structure you will need to have the model, in this case the ```best.pt``` file that is available in the ```weights``` directory in the ```model.tar.gz``` file we generated from our training and AMT jobs in this notebook earlier. To dive deep into creating this model directory structure refer to [Use PyTorch with SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb706eaf",
   "metadata": {},
   "source": [
    "Further guidance on deploying a YOLOv5 model can be found in this AWS Machine Learning blogpost about scaling [YOLOv5 inference with Amazon SageMaker endpoints and AWS Lambda](https://aws.amazon.com/blogs/machine-learning/scale-yolov5-inference-with-amazon-sagemaker-endpoints-and-aws-lambda/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0adda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
